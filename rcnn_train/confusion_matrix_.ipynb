{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizations saved in: C:\\Users\\Samarth\\Desktop\\polar3D\\rcnn\\visualizations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def load_labels(file_path):\n",
    "    \"\"\"Load labels from a text file.\"\"\"\n",
    "    labels = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            labels.append((int(parts[0]), float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4])))\n",
    "    return labels\n",
    "\n",
    "def iou(box1, box2):\n",
    "    \"\"\"Compute IoU between two bounding boxes.\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "def compute_confusion_matrix(ground_truth_dir, prediction_dir, labels, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix for object detection.\n",
    "    - ground_truth_dir: Directory containing ground truth label files.\n",
    "    - prediction_dir: Directory containing prediction label files.\n",
    "    - labels: List of class labels.\n",
    "    - iou_threshold: IoU threshold for matching.\n",
    "    \"\"\"\n",
    "    confusion_matrix = np.zeros((len(labels), len(labels)), dtype=int)\n",
    "\n",
    "    gt_files = sorted(os.listdir(ground_truth_dir))\n",
    "    pred_files = sorted(os.listdir(prediction_dir))\n",
    "\n",
    "    for gt_file, pred_file in zip(gt_files, pred_files):\n",
    "        gt_path = os.path.join(ground_truth_dir, gt_file)\n",
    "        pred_path = os.path.join(prediction_dir, pred_file)\n",
    "\n",
    "        ground_truths = load_labels(gt_path)\n",
    "        predictions = load_labels(pred_path)\n",
    "\n",
    "        for gt in ground_truths:\n",
    "            gt_matched = False\n",
    "            gt_class, gt_box = gt[0], gt[1:]\n",
    "            \n",
    "            for pred in predictions:\n",
    "                pred_class, pred_box = pred[0], pred[1:]\n",
    "                if iou(gt_box, pred_box) >= iou_threshold:\n",
    "                    if pred_class == gt_class:\n",
    "                        confusion_matrix[gt_class, pred_class] += 1  # True Positive\n",
    "                    else:\n",
    "                        confusion_matrix[gt_class, pred_class] += 1  # Misclassified\n",
    "                    gt_matched = True\n",
    "                    predictions.remove(pred)  # Remove matched prediction\n",
    "                    break\n",
    "            \n",
    "            if not gt_matched:\n",
    "                confusion_matrix[gt_class, gt_class] += 1  # False Negative\n",
    "        \n",
    "        # Remaining predictions are False Positives\n",
    "        for pred in predictions:\n",
    "            pred_class = pred[0]\n",
    "            confusion_matrix[0, pred_class] += 1  # Assume unmatched predictions are background FP\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "def calculate_metrics(confusion_matrix):\n",
    "    \"\"\"Calculate precision, recall, and accuracy from the confusion matrix.\"\"\"\n",
    "    tp = np.diag(confusion_matrix)\n",
    "    precision = np.divide(tp, np.sum(confusion_matrix, axis=0), out=np.zeros_like(tp, dtype=float), where=np.sum(confusion_matrix, axis=0) != 0)\n",
    "    recall = np.divide(tp, np.sum(confusion_matrix, axis=1), out=np.zeros_like(tp, dtype=float), where=np.sum(confusion_matrix, axis=1) != 0)\n",
    "    accuracy = np.sum(tp) / np.sum(confusion_matrix) if np.sum(confusion_matrix) > 0 else 0\n",
    "    f1_score = np.divide(2 * (precision * recall), (precision + recall), out=np.zeros_like(precision, dtype=float), where=(precision + recall) != 0)\n",
    "\n",
    "    return precision, recall, f1_score, accuracy\n",
    "\n",
    "\n",
    "def visualize_confusion_matrix(confusion_matrix, labels, output_path):\n",
    "    \"\"\"Visualize and save the confusion matrix.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=labels)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=True)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def visualize_metrics(metrics, labels, output_dir):\n",
    "    \"\"\"Visualize precision, recall, and accuracy.\"\"\"\n",
    "    precision, recall, f1_score, accuracy = metrics\n",
    "    metrics_dict = {\"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1_score}\n",
    "\n",
    "    for metric_name, metric_values in metrics_dict.items():\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.bar(labels, metric_values, color=\"skyblue\")\n",
    "        plt.title(f\"{metric_name} by Class\")\n",
    "        plt.xlabel(\"Class Labels\")\n",
    "        plt.ylabel(metric_name)\n",
    "        for i, value in enumerate(metric_values):\n",
    "            plt.text(i, value + 0.01, f\"{value:.2f}\", ha=\"center\")\n",
    "        output_path = os.path.join(output_dir, f\"{metric_name.lower()}_by_class.jpeg\")\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "\n",
    "    # Visualize overall accuracy\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar([\"Overall Accuracy\"], [accuracy], color=\"lightcoral\")\n",
    "    plt.title(\"Overall Accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.text(0, accuracy + 0.01, f\"{accuracy:.2f}\", ha=\"center\")\n",
    "    output_path = os.path.join(output_dir, \"overall_accuracy.jpeg\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "# Example usage\n",
    "ground_truth_dir = \"dataset_rcnn\\labels\\test\"  # Directory containing ground truth files\n",
    "prediction_dir = \"rcnn_train\\predictions\"    # Directory containing prediction files\n",
    "labels = [\"Background\", \"Rock\", \"Shadow\"]\n",
    "\n",
    "conf_matrix = compute_confusion_matrix(ground_truth_dir, prediction_dir, labels)\n",
    "metrics = calculate_metrics(conf_matrix)\n",
    "\n",
    "output_dir = \"rcnn_train\\visualizations\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "visualize_confusion_matrix(conf_matrix, labels, os.path.join(output_dir, \"confusion_matrix.jpeg\"))\n",
    "visualize_metrics(metrics, labels, output_dir)\n",
    "\n",
    "print(\"Visualizations saved in:\", output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
